\section{Analysis of Results} % (fold)
\label{cha:analysis_of_results}

The experiments and their results show a number of things: the approach of exemplar-NBNN detection works not only on the TUD dataset, but also fairly well on the VOC2007 set. Furthermore, the substitution of of the agglomerative clustering algorithm with a mode finding algorithm seems to have a positive effect, just like taking a value of $k>1$ and using LNBNN. Nevertheless there is much variance within the results, and therefore it is necessary to give a deeper analysis of the results, to find out how they can be interpreted.

\subsection{The Datasets} % (fold)
\label{sec:the_dataset}

The differences between the datasets are striking. The TUD Motorbikes set has a single class, and the test set contains only positive images (i.e. images with at least one motorbike). The training set has a clear-cut segmentation, which improves the feature quality. The VOC07 dataset is much harder. Every class appears in only 5\% of the test images, excluding the \emph{person} class, which occurs in 40\% of the images. To put it another way, in the TUD Motorbike test the task is for every image to locate the motorbike object(s), while in de VOC2007 test the task is for every image and every class to assess the possibility of this class having an object in the image, and if yes, to locate it.

Of course, in practice these tasks are modeled in the same way. The detections within a single class are all compared to one another in terms of their value (cluster size), across all images, both class and non-class images (\emph{Cf.} the PASCAL VOC evaluation method \cite{pascal-voc-2007}.)

Another important aspect is that the TUD dataset tends to have motorbike objects that are large, centered in the image, and with little clipping and occlusion. Some of the classes of VOC2007 consist of objects that are generally small (\emph{bottle}), not in the focal point of the image (\emph{chair, pottedplant}), or often occluded by other objects (\emph{diningtable}).

This makes it obvious that the scores overall are higher on the TUD Motorbikes test than on the VOC2007 test. It raises the question however whether exemplar-NBNN might perform relatively better, or worse, on the TUD motorbikes test than on the VOC2007 set. It is difficult to compare this to other methods, because the TUD set is not regularly used in the literature. It is possible however to visualize what kinds of objects in both tests are hard to detect, and which ones are easy. This might give more insight in the mechanics of the algorithm.

\todo[inline]{Give highest scoring hits for both methods (TP), highest scoring misses (FP), lowest of both. Give relationship between object size and AP, just like VOC does}

% subsection the_dataset (end)

\subsection{Clustering Algorithms} % (fold)
\label{sub:anal_clustering_algorithms}

Overall, the suspicion seems to be correct that quickshift should perform better than agglomerative clustering because of the more natural way of selecting a cluster center, i.e. a detection out of a cluster of hypotheses. The results however depend a lot on class, and are not always very obvious.

To see whether the clustering of hypotheses is needed at all, for some tests a comparison was made between the performance of the ranked detections versus ranking the hpyotheses. As a ranking method, Becker's $Q_H$ measure was used (Equation~\ref{eq:qh}). Figure~\ref{fig:hyprank}

\begin{figure}{hbt}
    \centering
    \missingfigure{Duration graph}
    % \includegraphics[width=0.8\textwidth]{hyprank}
    \caption{Comparison of Average Precision using the clustered detections versus the hypotheses before clustering, on a test with \ldots}
    \label{fig:hyprank}
\end{figure}
\todo{Make a figure with this.}

The recall of the detections will usually be lower than the recall when using hypotheses, because the detections are more or less a subset of hypotheses. However, ranking the hypotheses appears to be much harder than ranking detections, resulting in a much lower AP. This shows the main benefit of clustering.

% subsection clustering_algorithms (end)

\subsection{The Influence of $k$} % (fold)
\label{sub:the_influence_of_k_}

The experiments show that performance increases when multiple neighbors are taken into account, confirming the theory of descriptor aliasing in Section \ref{sec:descriptor_aliasing}. There is however a quite narrow sweet spot: setting $k$ too high hurts performance and, to an even larger extend, efficiency. \todo{write a small section on this sweet spot, and how k in LNBNN might ideally be higher, albeit time constraints}.


% subsection the_influence_of_k_ (end)

\subsection{Time and Memory} % (fold)
\label{sub:time_and_memory_constraints}

Detection algorithms can be compared not only for their qualitative performance, but also in terms of complexity and efficiency. Exemplar-NBNN is quite heavy in both terms. Given $t$ training images, $s$ test images, $d$ descriptors per image, on average, $c$ classes, and $k$ nearest neighbors, this is the time complexity breakdown of the main stages of the pipeline:
\begin{enumerate}
    \item Extracting features from the training images, splitting them into the classes, adding them to their respective NN indexes and saving their exemplars: $\frac{td}{c} \log \frac{td}{c}$, using FLANN kd-trees for building NN indexes.
    \item Extracting features from the test images: $s\times d$
    \item Finding $k$ nearest neighbors from each class, for each descriptor of each test image: $d s c \log \frac{td}{c} $, using kd-trees.
    \item Performing detection. For each test image and each class, get its hypotheses, calculated their pairwise overlap, and cluster them: $s c x\big((d k)^2\big)$, factor $x>1$ depending on the clustering algorithm, but at least once for the pairwise overlap.
\end{enumerate}

\begin{figure}{hbt}
    \centering
    \missingfigure{Duration graph}
    % \includegraphics[width=0.8\textwidth]{duration}
    \caption{Duration of various tests.}
    \label{fig:duration}
\end{figure}

\todo[inline]{explain and indicate what's important. Say something on memory complexity. Add something about no of descriptors in dsampling, compared with hlp.}

% subsection time_and_memory_constraints (end)


% section analysis_of_results (end)