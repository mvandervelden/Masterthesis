\section{Experiments} % (fold)
\label{cha:experimental_setup}
\todo[inline]{Don't forget to cover details like sampling method, features (separate subsection?), difference in settings for each test and data set, etc.

ALSO: INCLUDE RESULTS!}

The previously elaborated theory on local NBNN exemplar model detection was tested in a number of experiments. First, the NBNN detection experiments of Becker \cite{becker2012codebook} were replicated to validate the implementation, see Section \ref{sec:nbnn_detection}. These experiments were altered in a number of ways, to see the effects of different descriptors, feature selection rules, clustering algorithms and using Behmo's optimized NBNN approach (Section \ref{sec:nbnndet_improved}). In the final experiment, McCann's Local NBNN was integrated into the detection algorithm (Section \ref{sec:local_nbnn_detection}). Most tests were performed on both the TUDmotorbikes and VOC2007 benchmark detection sets.

\subsection{Features} % (fold)
\label{sec:features}

In most experiments, feature selection is done by dense sampling. Each point was sampled at various scales, to allow for scale invariance. The basic feature that has been used was the scale-invariant feature transform (SIFT). \cite{lowe2004distinctive}

\todo[inline]{Is it necessary to include rootsift/colorsift? No good results obtained with it}

% section features (end)

\subsection{PASCAL VOC Data Set} % (fold)

\label{sec:voc_data_set}
The Visual Object Classes (VOC) 2007 challenge of the PASCAL network \cite{pascal-voc-2007} provides a data set annotated for image detection. The data set consists of 20 object classes plus a background class, on a total of 9,963 images containing 24,640 annotated objects. The images are all taken from Flicker, and show a large variety of image quality and a large intra-class variety of objects. The data set is subdivided into 50\% test set, 25\% training set and 25\% validation set, with approximately equal distribution of class object distribution over the sets.

The VOC 2007 dataset is partly annotated for class segmentation, 844 images subdivided in the same way as the full set. This subset is used for the experiments below, in part because segmentation proved to be a good ground truth for descriptor selection, and also because of time and memory constrains both for training and testing the algorithms.

% section voc_data_set (end)

\subsection{TUD motorbikes Data Set} % (fold)
\label{sec:tudmotorbikes_data_set}
The TUD Motorbikes data set is a selection of motorbikes from different benchmark sets. \cite{fritz2005integrating} The training set consists of 153 images of motorbikes to a uniform background, and are segmented into motorbike and background. The test set consists of 115 images, all of motorbikes, with various backgrounds. Becker \emph{et al.} \cite{becker2012codebook} use this benchmark to test their setup. In the training phase they train on the motorbike features from the TUD train set, and randomly add features from non-object areas of the PASCAL VOC 2007 set, as background features. This same setup was used as a comparison to Becker's experiments.

% section tudmotorbikes_data_set (end)

% \subsection{Caltech101 Data Set} % (fold)
% \label{sec:caltech101_data_set}
% \todo[inline,color=red]{Not sure if used.}
% % section caltech101_data_set (end)
% 
% \subsection{Graz01 Data Set} % (fold)
% \label{sec:graz01_data_set}
% \todo[inline,color=red]{Not sure if used.}
% % section graz01_data_set (end)

% SKIP THIS PART
% \subsection{NBNN Classification} % (fold)
% \label{sec:nbnn-cls}
% \todo[inline,color=red]{This section only if the tests succeed. Most of the method is already in the NBNN section.}
% 
% % section nbnn-cls (end)

\subsection{Exemplar-NBNN Detection} % (fold)
\label{sec:nbnn_detection}
The detection experiment is more advanced than image classification. In the first place, not only a class should be determined, but a rectangular bounding box that indicates the object's location within the image. For exemplar based detection, these bounding boxes are generated from exemplars, which means that these have to be stored for all descriptors in the training images. Secondly, even though the object's background can supply information about the object's class, it should be viewed separately from the object itself, because it is needed to create a sharp boundary between object and background. Finally, in the detection task it is possible for an image to have multiple objects, of the same or different classes. Furthermore, these objects might overlap. This makes detection a more subtle task than classification, and quite some changes of the classification are needed to get a good detection algorithm.

For exemplar-NBNN detection, the ground-truth bounding boxes of the training images is used to create exemplars of the descriptors. Only descriptors sampled from the object itself can be used for the NN index of the object's class. The bounding box can be used to define this boundary, but when available, a segmentation ground truth can also be used as a basis, because bounding boxes tend to include quite some background area too. \todo{image?}

An exemplar is defined as a 4D vector of the relative width and height of the object's bounding box with respect to the descriptor's scale, and the relative horizontal and vertical position of this descriptor within the bounding box. In this way a new bounding box (called a hypothesis) can be reconstructed from a new descriptor location and scale.

Descriptors in the training images that fall outside the objects are regarded as background, and are added to a separate background class.

For each test image and each class, descriptors are densely sampled, and the distance of each descriptor towards the current class and towards the background class is measured using approximate NN. Now, all descriptors closer to the class than to the background are seen as potential detections of an object. From these descriptors, detection hypotheses are calculated (using their NN-descriptor's exemplar).

This array of hypotheses is clustered to find likely detections. The similarity measure used to find clusters is based on the area of overlap ($AO$) between two hypotheses:
\begin{equation}
    AO(H_a, H_b)= \frac{|H_a\cap H_b|}{|H_a\cup H_b|}
\end{equation}

The resulting detections can be ranked by the number of hypotheses that support it, or by the average relative distance between foreground and background NN of the descriptors on which the hypotheses were based.

\subsubsection{NBNN Detection Using Single Link Clustering} % (fold)
\label{sub:nbnn_detection_using_single_link_clustering}

\todo[inline]{Write a short piece on single link clustering, the parameters and stuff. Refer to Becker's paper}

% subsection nbnn_detection_using_single_link_clustering (end)

\subsubsection{NBNN Detection Using Quickshift Clustering} % (fold)
\label{sub:nbnn_detection_using_quickshift_clustering}

\todo[inline]{Write a short piece on quick shift clustering, the parameters and stuff}

% subsection nbnn_detection_using_quickshift_clustering (end)

\subsubsection{Exemplar-NBNN Results} % (fold)
\label{sub:exemplar_nbnn_results}
\todo[inline]{Do results}
\begin{figure}[hbt]
    \centering
    \missingfigure[figwidth=0.8\textwidth]{Result graph}
\end{figure}

% subsection exemplar_nbnn_results (end)

% section nbnn_detection (end)

\subsection{Local Exemplar-NBNN Detection} % (fold)
\label{sec:local_nbnn_detection}
A downside of the exemplar-NBNN method is the descriptor aliasing problem explained in Section \ref{sec:descriptor_aliasing}. This can be solved by taking the $k$ of $k$NN to be greater than 1. This means that the $k$ closest neighbors over all classes (the object class and the background class) are taken into account when constructing detection hypotheses. Within these $k$ nearest neighbors, the ones belonging to the object class that are closer than any neighbor in the background class, are transformed into detection hypotheses, avoiding the risk of disregarding very similar, although not most similar, exemplars when clustering.

\todo[inline]{Previous is implemented, coming part will hopefully be so.}

Another disadvantage of exemplar-NBNN is it returning many false positive detections for images not having any objects of the current object class. The reason for this is obvious, as all descriptors closer to the class than to the background will be regarded as hypotheses, and therefore even images without objects get reasonably well ranked detections. This problem gets larger as the data sets contains more object classes.

A solution for this is to not only compare the current object class with the background class, but to compare it with all other classes, to see what each descriptor in the test image looks most like. This results in a hypothesis selection step that chooses only descriptors that are closer to the current class than to any other.

This however would create another problem not present in the original approach. Some descriptors could be an indication for multiple classes, because certain parts of objects are quite similar. While the original exemplar-NBNN approach allows for this, regarding each class independently, this multi-class approach prevents this. Think of the similarity of a bicycle wheel with that of a motorbike or a car. To disregard all but one of these classes might cause very little evidence for most classes in an image, giving less stable detections. This reminds of Boiman's argument that much evidence is vital for the NBNN method, because the Naive Bayes assumption is only met towards infinity and there is no training phase to compensate for that.\cite{boiman2008defense}

At the same time, the goal to compare all classes simultaneously is similar to McCann's formulation of local NBNN's query.\cite{mccann2012local} (\emph{cf.} Section \ref{sec:local_nbnn}). Merging all class indexes into one, and taking into account not only the first per class, but the $k$ closest neighbors overall prevents having too many false positive results, while keeping the possibility of a descriptor to be used for a hypothesis for multiple classes like in the original problem.

As a bonus, local NBNN for detection also incorporates a way to prevent the aliasing problem. Among the $k$ closest neighbors, there might be multiple ones from the same class. Instead of disregarding these neighbors like local NBNN does, they can be taken into account by creating hypotheses for all non-background nearest neighbors.

\todo[inline]{Some more?? And Results}

\begin{figure}[hbt]
    \centering
    \missingfigure[figwidth=0.8\textwidth]{Result graph}
\end{figure}

% section local_nbnn_detection (end)

\subsection{Training Weighted Distances} % (fold)
\label{sec:training_weighted_distances}

In image classification, the classes are generally quite well balanced. The amount of images in every class is usually approximately the same and all image are usually of the same size, making the assumption of a uniform prior hold. Therefore the natural distribution of descriptors in feature-space can be assumed to be equally sampled in every class. This makes it acceptable to use a learning method like optimal NBNN\cite{behmo2010towards} or \ldots \todo{put Wang's method here}\cite{wang2011improved} to tune the likelihood of classes to the local density in feature space to be equal for all classes.

In object detection, the sampling rate will not be equal over classes, especially the background class will have a larger sampling rate, simply because it will occur in virtually all images. The equal priors assumption therefore does not hold. This flaw is mitigated by the fact that a higher number of sampled descriptors also tends to make the feature space for that class more dense, and more likely to be the nearest neighbor.

The risk however is that the estimation of the feature space may differ largely per class. Classes with a large intra-class variety of descriptors, but with generally small objects will be sampled much sparser than classes with less descriptor variety and larger objects. Learning parameters to tune the feature space density might therefore result in overfitting for sparsely sampled classes or classes with a very high intra-class diversity.

This experiment will test this hypothesis using Behmo's optimal NBNN linear program to train per-class parameters on sampled images.

\todo[inline]{more detail about implementation of Behmo. Perhaps put the first part either to conclusion, or to theory?}

\begin{figure}[hbt]
    \centering
    \missingfigure[figwidth=0.8\textwidth]{Result graph}
\end{figure}

% section training_weighted_distances (end)

% chapter experimental_setup (end)