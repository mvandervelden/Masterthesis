
\chapter{Related Work} % (fold)
\label{sec:related_work}
\todo[inline] {Needs revision and expansion (shorten the obvious parts, expand on interesting NBNN \& detection related papers)}

\section{Image Descriptors} % (fold)
\label{sub:image_descriptors}
To be able to make a description of each object class, a representation of each (part of an) image is needed that best captures the informative aspects of the objects. The most basic descriptor type is to use the pixel values of the image. These capture the image exactly and might therefore seem very useful as descriptors of object types. The problem however with pixel values, is that images tend to vary wildly in them, even though the exact same object might pictured.

There are a number of types of variation that may be expected when comparing arbitrary images of the same object type:
\begin{description}
    \item[Camera] Use of different cameras and compression algorithms (image filetypes) will cause a change of pixel values. Images might be darker or lighter, have a different balance of colors, more or less color depth, etc.
    \item[Scene] Lighting of the scene may cause significant changes in pixel values, because of the change in intensity or in spectrum. Also, the background behind the object might change
    \item[Orientation] Transformation of the object with respect to the image frame. An object might be on an arbitrary location in the image, rotated in various angles, viewed from up close or from far away.
    \item[Occlusion] Objects might be partially out of sight, because they are not framed entirely, or because something is in front of them.
    \item[Intra-class-variation] Objects from a single class may vary wildly in appearance. Imagine a number of objects that qualify as a chair, and how different these might look.
\end{description}
Therefore, raw pixel values might not be a very good choice as description on its own
\todo[inline]{connect this with other descriptors, be shorter than now, focus on sift as that's what I'll use, give SIFT's advantages and limitations. Meaning TODO: revision of the following}

In general, there are three kinds of descriptor types: global, regional and local descriptors. Pixel values are a type of local descriptors. These descriptors capture the local information (in this case at pixel level) very well, but they don't take their context into account. This is what regional descriptors do. They describe in a concise way what is to be derived from a small region of pixels in an image. Usually, these describe local changes in intensity, edges, lines, corners or blobs, and are very popular in computer vision. Global features describe characteristics of the image as a whole. 

Very often, multiple kinds of descriptors are used together to make a description of an image, and of objects.

% subsection image_descriptors (end)

\section{Training Methods} % (fold)
\label{sub:training_methods}
\todo[inline]{TODO revision of this section and the next into one kind of related work on detection/classification section?}

\todo[inline]{Better be incorporated in the previous subsections, otherwise it will be too much repetition... small piece on BoF and similar methods. small piece on the following subsection. Perhaps at the end of the NBNN section, as `other classification methods'??}

\todo[inline]{(perhaps diverge a little to other methods that use NN and detection, but in a different way (SVM-kNN, galleguillos, etc.), !related work section!, but I dropped that, lets see later on).}


With the representation of images, it becomes possible to make a model for each object class to be detected. In this way, test images can be matched with each class model to find out which is the most likely.

To be able to make a class description, sample images are needed, annotated with a description of what the image represents. This is the training set. From this set, the descriptors found can be labeled with the correct class, so a description of each can be made. Usually this is done in a learning phase.


Bag of Words is another popular method \cite{lazebnik2006beyond, van2011exploiting} \todo[fancyline]{continue this as an overview}

% subsection training_methods (end)
\section{Detection methods} % (fold)
\label{sub:detection_methods}
With a choice of image descriptors, a class description and a training method, image classification can be performed. For object localization within an image however, a strategy is needed to estimate classifications for parts of images instead of images as a whole.

The most straightforward method is to take a window on a test image, and to classify that part of the image as if it were an image in itself. This sliding window approach \todo[fancyline]{reference} is just a small conceptual step, but generates a lot of possible windows to be classified, therefore costing much computation. This hampers the applicability of this naive method. Smarter versions of this approach include include a divide-and-conquer tactic called branch-and-bound search. \cite{lampert2008beyond} In this method, a fitness (response) of the current window is calculated, and then the window is subdivided in two non-overlapping windows, the most promising of which is iteratively subdivided until the globally highest fitness is found. However being much faster than the vanilla sliding window approach, it has the disadvantage of covering hard boundaries between sub-windows, making it hard to find exact locations of objects.

Part-based models have a different approach. This method models classes with special regional descriptors which define the relative orientation and scale of parts of an object with respect to the location and size of the object as a whole. \cite{leibe2004combined, chum2007exemplar, felzenszwalb2010object} These descriptors enable the detection algorithm to look for the arrangement of descriptors in a test image, and to define the most likely locations of objects. Some methods expand this into trying to create 3D models of classes, to make matching rotation independent. \todo[fancyline]{reference}
% subsection detection_methods (end)

% chapter related_work (end)